{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from wrangle import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up an example scenario as perspective for our regression exercises using the Zillow dataset.\n",
    "\n",
    ">As a Codeup data science graduate, you want to show off your skills to the Zillow data science team in hopes of getting an interview for a position you saw pop up on LinkedIn. You thought it might look impressive to build an end-to-end project in which you use some of their Kaggle data to predict property values using some of their available features; who knows, you might even do some feature engineering to blow them away. Your goal is to predict the values of single unit properties using the obervations from 2017.\n",
    "\n",
    ">In these exercises, you will complete the first step toward the above goal: acquire and prepare the necessary Zillow data from the zillow database in the Codeup database server.\n",
    "\n",
    "1. Acquire bedroomcnt, bathroomcnt, calculatedfinishedsquarefeet, taxvaluedollarcnt, yearbuilt, taxamount, and fips from the zillow database for all 'Single Family Residential' properties.\n",
    "\n",
    "2. Using your acquired Zillow data, walk through the summarization and cleaning steps in your wrangle.ipynb file like we did above. You may handle the missing values however you feel is appropriate and meaningful; remember to document your process and decisions using markdown and code commenting where helpful.\n",
    "\n",
    "3. Store all of the necessary functions to automate your process from acquiring the data to returning a cleaned dataframe with no missing values in your wrangle.py file. Name your final function wrangle_zillow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema='zillow'\n",
    "\n",
    "# sql_database_info_probe(schema)\n",
    "\n",
    "\n",
    "# query='''\n",
    "# select * from propertylandusetype\n",
    "\n",
    "\n",
    "# '''\n",
    "# read=pd.read_sql(query, get_db_url(schema))\n",
    "# read\n",
    "\n",
    "\n",
    "\n",
    "def sql_database_info_probe(schema_input):\n",
    "    '''\n",
    "    returns a list of tables\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    schema = schema_input\n",
    "\n",
    "    query_1 = f'''\n",
    "    SELECT table_schema \"Database Name\",\n",
    "    ROUND(SUM(data_length + index_length) / 1024 / 1024, 4) \"DB Size in (MB)\" \n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema= \"{schema}\" \n",
    "    GROUP BY table_schema\n",
    "    ;\n",
    "    '''\n",
    "\n",
    "    query_2 = f'''\n",
    "    SELECT table_name AS \"Tables\",\n",
    "    ROUND(((data_length + index_length) / 1024 / 1024), 4) AS \"Size (MB)\"\n",
    "    FROM information_schema.TABLES\n",
    "    WHERE table_schema = \"{schema}\"\n",
    "    ORDER BY (data_length + index_length) DESC;\n",
    "    '''\n",
    "\n",
    "    info1 = pd.read_sql(query_1, get_db_url(schema))\n",
    "    info2 = pd.read_sql(query_2, get_db_url(schema))\n",
    "\n",
    "    display(f'In {schema} your overlall size(MB) is:', info1)\n",
    "    tablenames = [x[0] for x in [list(i) for i in info2.values]]\n",
    "    display(\n",
    "        f'In {schema} you have the following table names and their sizes:', info2)\n",
    "    x = [(pd.read_sql(f'describe {x}', get_db_url(schema)))for x in tablenames]\n",
    "    [display(sympify(f'{(tablenames[i]).capitalize()}'), k)\n",
    "     for i, k in enumerate(x)]\n",
    "    y = [(i.Field).str.split() for i in x]\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "schema_input='zillow'\n",
    "\n",
    "# sql_database_info_probe(schema_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google the data dictionary for this data set it was used in a kaggle competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=prep_zillow_2017()\n",
    "\n",
    "df.nunique().sort_values()\n",
    "\n",
    "cols=set(df.columns.to_list())\n",
    "nums=cols-{\n",
    "'fips',                          \n",
    "'bedroomcnt',                   \n",
    "'bathroomcnt',\n",
    "'decade'}\n",
    "cats=cols-nums\n",
    "\n",
    "cols=list(cols)\n",
    "\n",
    "cats=list(cats)\n",
    "nums=list(nums)\n",
    "\n",
    "df.head()\n",
    "df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows histograms of the categorical data\n",
    "\n",
    "\n",
    "# for i in (cats):\n",
    "\n",
    "  \n",
    "\n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     sns.displot(data=df,x=df[i],discrete=True)\n",
    "    \n",
    "    \n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in (nums):  \n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     sns.displot(data=df,x=df[i],stat='density',kde=True,discrete=False)\n",
    "    \n",
    "    \n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maybe this a good subset to learn on as we are extrem\n",
    "df1=df.apply(stats.zscore)\n",
    "df1=df1.apply(abs)\n",
    "df1=df1.applymap(lambda x: x<=1.25 )\n",
    "df2=df[df1]\n",
    "df2.nunique(dropna=True)\n",
    "\n",
    "df2.isna()\n",
    "df2.dropna(inplace=True)\n",
    "df2\n",
    "a=len(df)\n",
    "b=len(df2)\n",
    "x=(b-a)/a\n",
    "x=f'{x*100:.2f}%'\n",
    "\n",
    "\n",
    "display(print(f'This is our percent change after removing all the outliers and then the nulls:\\n {x}'),df2.skew(),df2.kurt(),df2.kurt().mean())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this is the future to make categories binned by the z score. Which would be useful to explicitly identify outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3=get_zillow_2017()\n",
    "df3=df2\n",
    "a=len(df3)\n",
    "df3cols=df3.columns.to_list()\n",
    "df3=remove_outliers_v2(df=df3, k=1.25, col_list=df3cols)\n",
    "df3.head()\n",
    "\n",
    "df3.isna()\n",
    "df3.dropna(inplace=True)\n",
    "df3\n",
    "\n",
    "b=len(df3)\n",
    "x=(b-a)/a\n",
    "x=f'{x*100:.2f}%'\n",
    "\n",
    "\n",
    "display(print(f'This is our percent change after removing all the outliers and then the nulls:\\n {x}'),df3.skew(),df3.kurt(),df3.kurt().mean())\n",
    "\n",
    "\n",
    "##here I compared two different ways to exclude outliers by taking the mean of their respective kurtosis by column. Note the two means are nearly the same. I did not take the abs since negative kurtosis is meaningful, Tukey \n",
    "tukey_wnorm_zscore_selection=-0.6661175115340092\n",
    "tukeyalone=-0.24357716836225213\n",
    "zscore_selection_alone=-0.24166069249521904\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look 1.25 stand deviations\n",
    "for i in (cols):\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.displot(data=df2,x=df2[i],stat='density',kde=True,discrete=False)\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.kurtosis()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " >The normalized graphs (commented out for perfomance), skew and kurtosis show we have to consider some outliers in our data likely need to scale to obtain interesting results.\n",
    "\n",
    " >At this step we have simply droped the NAN's as they were a marginal ammount of data and added a categorical colum for decades. The graphs are  for the non categorical cols are not interesting at this step as the highlighted by the extreme skew and kurtosis.\n",
    "\n",
    "\n",
    "  >FIPS (a unique county identifier code) so it might not be useful othere than to clasify which county this was in\n",
    "\n",
    "\n",
    "\n",
    "In the lesson review our instructor created a fuction to delete the outliers in each column. Tukey method https://en.wikipedia.org/wiki/Tukey%27s_range_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
